---
title: "Data Processing & Management"
---

### Acoustic-based Indices

Comparing acoustic data from different regions and providers can be complex due to variations in sampling rate, file duration, and file naming conventions. Differing sampling rates can affect the resolution and frequency content of recordings, while variations in file duration may influence the amount of data available for analysis. Additionally, inconsistencies in file naming conventions can hinder data organization and interoperability, requiring careful preprocessing and standardization efforts to ensure accurate and meaningful comparisons across datasets.

The following steps were required to produce acoustic-based index variables:

-   **Audio File Naming Convention**: scikit-maad extracts date and time information from audio file names and can accept one of two formats. For this project, we proceeded with the SM4 format, which consists of some prefix, followed by the date starting with year and time (e.g., PREFIX_YYYYMMDD_HHMMSS.wav). All datasets required audio file renaming in order to extract temporal information for plotting these time-series data. Per-site scripts generated for renaming files generated renamed copies of the data (to ensure originals were retained) are provided in the GitHub repository.

-   **File Segmentation**: Another challenge presented during this project was the limitations of processing data with large audio files, particularly the sanctuary data. This in addition to some programmatic challenges at calculating indices at specific durations, resulted in the need to segment audio files into smaller durations. For all sanctuary sites and the Chuckchi Sea, audio files were segmented into 5-minute files, which was the largest duration for all datasets. Although a graphical user interface (GUI) was developed during this study to allow for user-defined index calculations, this did not work for all datasets, so several required further segmentation into 2.5-minute files. Note that 120 and 150-second files are considered the same file duration as reducing the file duration from five minutes to smaller files requires 150-second interval.

-   **Processing Limitations**: Processing a suite of 58 index variables for a month of data at a time is a computationally intensive process. As an example, processing one month of data with a 64 kHz sampling rate and 5-min interval at full bandwidth on a Windows 11 Desktop with an Intel Core i7-10700 CPU @2.9 GHz and 32 GB of RAM took an average of two and a half days to complete. Processing at the 30-second and 10-second intervals was not feasible for datasets with continuous, five-minute files. In an attempt to streamline the process by testing out an Amazon Warehouse Service (AWS) EC2 virtual machine instance, uploading a week of data took approximately 16 hours. We, therefore, did not proceed with the smallest duration index calculation for this exploratory project but did evaluate 120/150-second durations for six of the eight datasets, and 300 seconds for five of the eight datasets. Due to smaller file sizes, the May River and Key West datasets were processed at the 10-second duration for 16 kHz bandwidth runs in addition to their native durations for the month of February. Potential next steps and solutions to processing limitations are discussed in the "Recommendations" section.

-   **Decimation**: All datasets (aside from Chuckchi Sea) required decimation to the lowest sampling rate (16 kHz, which is the sampling rate for Chuckchi Sea) for comparison at a common spectral bandwidth. The OSA EZ Decimator was used to generate separate downsampled datasets from the folder of renamed files.

-   **Processing Stages**: The processing of each dataset included several runs using settings in the scikit-Maad batch processing code for either a full bandwidth or 16 kHz bandwidth. The code first requires manual setting of the hydrophone calibration and gain settings, then requires the user to determine the low, medium, and high spectral band thresholds. These included:

    -   Full Bandwidth: 0-1,500 Hz (low); 1,500-8,000 Hz (med); 8,000 - Nyquist (high)

    -   16 kHz Bandwidth: 0-1,000 Hz (low); 1,000-4,000 Hz (med); 4,000-8,000 Hz (high)

    An FFT rate is required for user selection, and a test of Key West indicated longer processing times for larger FFT's without an effect on the trends of the index values. Therefore an FFT of 512 was selected for all processing.

-   **Annotations**: Several datasets provided annotations that were used in the evaluation of acoustic indices. The greatest resolution in annotations of underwater biophony was from the Key West and May River datasets, resulting in their highlight on the "Annotations" tab of the dashboard. The sanctuary sites (Gray's Reef and Hawaii) provided an annotation dataset of cetacean occurrence but on an hourly level, which was too low a resolution to derive meaningful indices associations. Vessel annotations were available for four of the eight datasets. Triton, the analytical tool used to identify passing vessels was used for annotation of vessels in all remaining datasets with continuous audio. There were limitations to this as the automated Ship Detector remora This mean that the only dataset without vessel information is the Caesar Creek dataset, as the short duration of audio makes the confirmation of a passing vessel challenging.

------------------------------------------------------------------------

### Environmental Data using SeascapeR

Buffers created with 0.55 degrees which works out to an approximate 60 km by 60 km regions surrounding each hydrophone.

![](images/Buffers_0.55deg.png){fig-align="center"}

------------------------------------------------------------------------

### Data Compilation & Collation

Compiling acoustic index variable data from multiple sites, each with varying durations and sampling rates, was essential for subsequent data summarization and visualization. Furthermore, consolidating annotation information, vessel occurrences at each site, and water class data is necessary to provide comprehensive context for the acoustic recordings and facilitate meaningful analysis.
