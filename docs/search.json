[
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "BioSound Exploratory Project",
    "section": "",
    "text": "The BioSound Working Group\nThe BioSound working group is composed of various scientists and stakeholders interested in producing acoustic-based data products that support marine biodiversity monitoring and conservation. BioSound is particularly interested in understanding the value of underwater soundscape metrics as indicators of biological and anthropogenic change in the context of biodiversity conservation. BioSound was formed as an initiative of the Marine Biodiversity Observation Network (MBON). Additional information regarding the goals and objectives of the group can be found at the MBON BioSound site. This work was supported by the U.S. Marine Biodiversity Observation Network (MBON) co-organized by NOAA, NASA, BOEM, and ONR through the National Oceanographic Partnership Program (NOPP). \n\n\nExploratory Project\nThe group’s initial project begins with a focused initiative to compare acoustic-based biodiversity indices across various ocean environments, paving the way for a broader, more extensive analysis. These efforts aim to generate valuable data products highlighting biological sounds and habitat characteristics in regional ocean and coastal areas, while also providing recommendations for standardizing soundscape metric methods and enhancing standards within the bioacoustic community.\nExplore the data collected during this exploratory study via our dashboard:\n\nBioSound Data Exploration Dashboard\nThe analytical workflow for multiple components begins with collecting raw audio files from 8 distinct datasets. These files undergo comprehensive data management steps, including preprocessing and standardization, to enable the extraction of acoustic indices in multiple comparable ways across datasets. Additionally, annotations of vessel activity and, where feasible, organismal occurrences are integrated with environmental data to provide context for the acoustic recordings. Subsequently, the processed data are incorporated into a user-interactive dashboard designed for evaluating data across sites and indices. This dashboard facilitates comprehensive exploration and analysis, enabling researchers to gain insights into the relationships between acoustic indices, environmental factors, and biological activity across diverse marine environments. Additionally, scripts and data products obtained during this analysis are available in the Ocean Science Analytics BioSound-Exploratory-Project GitHub site.\n\n\n\n\n\n\n\n\nThe Datasets\nThe datasets utilized in this study originate from diverse environments, including offshore, mangrove, and reef ecosystems, spanning both the Pacific and Atlantic Oceans. These datasets have been contributed by members of the BioSound working group, highlighting a collaborative effort to incorporate acoustic data from various marine habitats for comprehensive analysis. Below, a map and table delineate the specific datasets employed in this study, providing geographic context and detailed information about each dataset’s characteristics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Name\nSource\nContact\nMarine Ecosystem Type\nNative Sampling Rate (kHz)\nDuty Cycle\nNative File Duration\nSelected Months\n\n\n\n\nBiscayne Bay, FL\nUniversity of Miami\nNeil Hammerschlag/Abby Tinari\nMangrove\n32\n10 seconds every minute\n0:00:10\nFeb - Mar, 2019\n\n\nChukchi Sea, Hanna Shoal\nOregon Sate University\nKate Stafford\nOffshore\n16.384\n25 minutes every hour\n0:25:00\nJan - Mar, 2019\n\n\nGray’s Reef, GA\nNEFSC-SanctSound\nTim Rowell\nOffshore\n48\nContinuous\n~5:40:00\nJan - Feb, 2019\n\n\nKey West, FL\nFlorida Fish & Wildlife\nJessica Keller\nCoral Reef\n48\n30 seconds every five minutes\n0:00:30\nFeb - Mar, 2020\n\n\nMay River, SC\nU. of South Carolina, Beaufort\nAlyssa Marian/ Eric Montie\nEstuary\n80\n2 minutes every 20 minutes\n0:02:00\nJan - Mar, 2019\n\n\nOlowalu (Maui, HI)\nSanctSound - Hawaiian Islands Humpback NMS\nEden Zang\nIsland/Nearshore\n48\nContinuous\n~5:40:00\nJan - Feb, 2019\n\n\nONC-MEF\nOcean Networks Canada\nJasper Kanes\nOffshore\n64\nContinuous\n0:05:00\nJan - Feb, 2019\n\n\nOOI-HYDBBA106\nOcean Observatories Initiative\nLiz Ferguson\nShelf\n64\nContinuous\n0:05:00\nJan - Feb, 2019\n\n\n\n\n\nAcoustic-based Indices\nWe utilized scikit-maad, a comprehensive Python package tailored for acoustic analysis, to efficiently extract acoustic-based indices from our audio recordings. Its rich set of functionalities and easy-to-use interface provided us with the necessary tools to process and analyze acoustic data, enabling robust and standardized computation of biodiversity indices. An description of all indices is available from the “Acoustic Indices” tab, and required processing is indicated in the “Data Processing & Management” tab.\n\n\n\n\n\n\n\nEnvironmental Data\nSeascapeR, a powerful tool developed by the Marine Biodiversity Observation Network (MBON), offers extensive capabilities for analyzing and visualizing marine habitat data. Leveraging SeascapeR, we are conducting comparative analyses between distinct water classes and acoustic indices, allowing us to explore the relationships between habitat characteristics and potential acoustic biodiversity across diverse marine environments. Combinations of remotely sensed data variables at certain thresholds are used to characterize water bodies into one of 33 Seascape Global Water Classes.\n\n\n\nDashboard and Documentation\nOcean Science Analytics (OSA) spearheads the analytical effort, employing advanced data analytics techniques to derive meaningful insights from diverse marine datasets. For the data compilation and visualization piece, OSA and Waveform Analytics collaborated closely on the development of an interactive dashboard, data integration, and comprehensive documentation to facilitate intuitive exploration and interpretation of the integrated acoustic and environmental data.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data Processing & Management",
    "section": "",
    "text": "Acoustic-based Indices\nComparing acoustic data from different regions and providers can be complex due to variations in sampling rate, file duration, and file naming conventions. Differing sampling rates can affect the resolution and frequency content of recordings, while variations in file duration may influence the amount of data available for analysis. Additionally, inconsistencies in file naming conventions can hinder data organization and interoperability, requiring careful preprocessing and standardization efforts to ensure accurate and meaningful comparisons across datasets. The csv outputs from scikit-maad processing can be found in the “Acoustic Indices” folder on this documentation GitHub site.\nThe following steps were required to produce acoustic-based index variables:\n\nAudio File Naming Convention: scikit-maad extracts date and time information from audio file names and can accept one of two formats. For this project, we proceeded with the SM4 format, which consists of some prefix, followed by the date starting with year and time (e.g., PREFIX_YYYYMMDD_HHMMSS.wav). All datasets required audio file renaming in order to extract temporal information for plotting these time-series data. Per-site scripts generated for renaming files generated renamed copies of the data (to ensure originals were retained) are provided in the “DataManagement_Code” folder on the GitHub repository.\nFile Segmentation: Another challenge presented during this project was the limitations of processing data with large audio files, particularly the sanctuary data. This in addition to some programmatic challenges at calculating indices at specific durations, resulted in the need to segment audio files into smaller durations. For all sanctuary sites and the Chuckchi Sea, audio files were segmented into 5-minute files, which was the largest duration for all datasets. Although a graphical user interface (GUI) was developed during this study to allow for user-defined index calculations, this did not work for all datasets, so several required further segmentation into 2.5-minute files. Note that 120 and 150-second files are considered the same file duration as reducing the file duration from five minutes to smaller files requires 150-second interval. The python code for segmenting wav files can be found in the “DataManagement_Code” folder.\nProcessing Limitations: Processing a suite of 58 index variables for a month of data at a time is a computationally intensive process. As an example, processing one month of data with a 64 kHz sampling rate and 5-min interval at full bandwidth on a Windows 11 Desktop with an Intel Core i7-10700 CPU @2.9 GHz and 32 GB of RAM took an average of two and a half days to complete. Processing at the 30-second and 10-second intervals was not feasible for datasets with continuous, five-minute files. In an attempt to streamline the process by testing out an Amazon Warehouse Service (AWS) EC2 virtual machine instance, uploading a week of data took approximately 16 hours. We, therefore, did not proceed with the smallest duration index calculation for this exploratory project but did evaluate 120/150-second durations for six of the eight datasets, and 300 seconds for five of the eight datasets. Due to smaller file sizes, the May River and Key West datasets were processed at the 10-second duration for 16 kHz bandwidth runs in addition to their native durations for the month of February. Potential next steps and solutions to processing limitations are discussed in the “Recommendations” section.\nDecimation: All datasets (aside from Chuckchi Sea) required decimation to the lowest sampling rate (16 kHz, which is the sampling rate for Chuckchi Sea) for comparison at a common spectral bandwidth. The OSA EZ-Decimator was used to generate separate downsampled datasets from the folder of renamed files.\nProcessing Stages: The processing of each dataset included several runs using settings in the scikit-Maad batch processing code for either a full bandwidth or 16 kHz bandwidth. The code first requires manual setting of the hydrophone calibration and gain settings, then requires the user to determine the low, medium, and high spectral band thresholds. These included:\n\nFull Bandwidth: 0-1,500 Hz (low); 1,500-8,000 Hz (med); 8,000 - Nyquist (high)\n16 kHz Bandwidth: 0-1,000 Hz (low); 1,000-4,000 Hz (med); 4,000-8,000 Hz (high)\n\nAn FFT rate is required for user selection, and a test of Key West indicated longer processing times for larger FFT’s without an effect on the trends of the index values. Therefore an FFT of 512 was selected for all processing.\nAnnotations: Several datasets provided annotations that were used in the evaluation of acoustic indices. The greatest resolution in annotations of underwater biophony was from the Key West and May River datasets, resulting in their highlight on the “Annotations” tab of the dashboard. The sanctuary sites (Gray’s Reef and Hawaii) provided an annotation dataset of cetacean occurrence but on an hourly level, which was too low a resolution to derive meaningful indices associations. Vessel annotations were available for four of the eight datasets. Triton, the analytical tool used to identify passing vessels was used for annotation of vessels in all remaining datasets with continuous audio. There were limitations to this as the automated Ship Detector remora resulted in a significant amount of false positives (from the Chuckchi dataset), or missed several harder to discern vessels (e.g., from the ONC-MEF dataset, presumably as it is at a depth of over 2,000 meters). For three of the datasets, vessels were annotated in Triton as was recommended by documentation. Additionally, this tool does not work as well with short duration files, so we were unable to obtain vessel information from the Caesar Creek dataset.\n\nThe GUI-enabled batch processing code for scikit-maad is available on the OSA scikit-maad-biosound repository.\n\n\n\nEnvironmental Data using SeascapeR\nUsing remotely sensed data for accessing environmental information for these study areas required incorporating multiple variables into an appropriate format for comparing with time series acoustic indices data. The MBON SeascapeR tool was selected due to the water class designations that combine ocean color products (e.g. sea surface temperature, CHLA, etc.) of varying thresholds into a single category.\n\n\n\n\n\nThe seascapeR code natively queries a database of sanctuary shapefiles, which are used to define the region for which the classes are determined. The grid resolution within a shapefile is set to 0.05 x 0.05 degrees in a geographic coordinate system, providing water class values within approximately 5.6 km cells (1 degree = ~ 111.1 km at the equator, shrinking towards the poles). The code natively queries monthly composite data from ERDDAP (Metadata), for each 5.6 km cell within the provided sanctuary shapefile:\n\n\nModifications for BioSound Use\nThe sites used for BioSound included both sanctuary and non-sanctuary sites and required greater resolution in the ocean color data composites. The following steps were taken to obtain seascapeR water class data for each of the sites used in this study. The modified seascapeR code for this project is available on OSA’s seascapeR-biosound repository.\n\nSeascapeR code was modified to select 8-day composites (Metadata) of water class data, the highest resolution possible that provided gridded data to the fullest extent possible for each site.\nCustom shapefiles were required to be able to obtain water class data for each of the sites. Given the need to define comparable shapefiles for each of the study areas, a standard shape file was generated. Buffers of 0.55 squared degrees were created surrounding each hydrophone’s location resulting in approximately 60 km by 60 km shapefiles. Shapefiles overlapped with land for several sites, resulting in a lower sample size, however the larger shapefile region still resulted in sufficient coverage for each site.\nSeascapeR was used to extract water class percentages for each dataset, and written to csv files for incorporation with other data products.\n\n\n\n\n\n\n\n\n\n\nData Compilation & Collation\nCompiling acoustic index variable data from multiple sites, each with varying durations and sampling rates, was essential for subsequent data summarization and visualization. Furthermore, consolidating annotation information, vessel occurrences at each site, and water class data is necessary to provide comprehensive context for the acoustic recordings and facilitate meaningful analysis.\nThe dashboard is built using R Shiny. The acoustic indices, water class data, and annotations data are prepared in Python and stored in a duckdb database (stored locally due to file size, and on OSA’s BioSound Google Drive site for data products). All of the code used in the preparation of the data and dashboard can be found at the osa-mbon GitHub repository associated with Waveform-Analytics.",
    "crumbs": [
      "Data Processing & Management"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "indices.html",
    "href": "indices.html",
    "title": "Acoustic-Based Biodiversity Indices",
    "section": "",
    "text": "Soundscape Metrics\nSoundscape measurements refer to the quantitative analysis of the sounds present in a particular environment, capturing not only individual sounds but also their patterns, frequencies, and intensities over time. These measurements are important in relation to biodiversity indices for several reasons:\n\nIndicator of Biodiversity: The diversity and composition of species within an ecosystem can be inferred from the soundscape. Different species produce distinct sounds, and the presence or absence of certain sounds can indicate the presence or absence of particular species. For example, the diversity of bird calls in a forest can be used as an indicator of avian biodiversity.\nNon-invasive Monitoring: Soundscape measurements offer a non-invasive way to monitor biodiversity. Unlike traditional methods such as trapping or visual surveys, which can be disruptive and time-consuming, recording and analyzing soundscape data can be done remotely and continuously, providing valuable insights into biodiversity without disturbing the ecosystem.\nTemporal and Spatial Dynamics: Soundscape measurements capture the temporal and spatial dynamics of biodiversity. By analyzing changes in sound patterns and intensity over time and across different locations within an ecosystem, researchers can identify patterns of species activity, migration, and distribution, providing a more comprehensive understanding of biodiversity dynamics.\nEarly Warning System: Changes in the soundscape can serve as an early warning system for ecosystem health. Alterations in sound patterns, such as the decline or disappearance of certain species’ vocalizations, can indicate disturbances or threats to biodiversity, such as habitat loss, pollution, or invasive species encroachment, allowing for timely intervention and conservation efforts.\nIntegration with Biodiversity Indices: Soundscape measurements can be integrated with traditional biodiversity indices to enhance their accuracy and comprehensiveness. By combining acoustic data with other ecological data, such as species abundance and habitat characteristics, researchers can develop more robust biodiversity assessments that capture the full spectrum of biodiversity within an ecosystem.\n\n\n\n\n\n\n\n\n\nAcoustic-based Biodiversity Indices\nFor this exploratory study we use the package scikit-maad, a tool commonly used in the terrestrial community. Scikit-maad, short for “Machine learning for Acoustic Activity Detection,” is a Python package designed for the analysis of acoustic data, particularly for bioacoustic applications. It provides a comprehensive set of tools and functions for processing, extracting features from, and analyzing soundscape recordings. Scikit-maad is primarily focused on facilitating the use of machine learning techniques for tasks such as acoustic event detection, species classification, and biodiversity monitoring.\nThe developers of scikit-maad are a team of researchers and developers passionate about bioacoustics and computational ecology. They have expertise in signal processing, machine learning, and ecology, and their aim is to provide accessible and powerful tools for analyzing acoustic data and advancing research in the field of bioacoustics. The development of scikit-maad is supported by contributions from the open-source community and collaborations with researchers and practitioners in various domains related to acoustic ecology and biodiversity monitoring.\nIn scikit-maad, the acoustic-based indices can be categorized into several main categories based on the aspects of the acoustic signal they capture:\n\nAmplitude Indices:\nDescription: Indices that measure variations in amplitude.\nIncluded Indices: BGNf, KURTt, LEQf, MEANf, SNRf, SNRt\nReason: These indices evaluate the intensity and variability of sound amplitudes, reflecting the loudness and dynamic range of the soundscape.\nComplexity Indices:\nDescription: Indices that measure the complexity of the soundscape, incorporating spectral, temporal and amplitude factors.\nIncluded Indices: ACI, ACTspCount, ACTspFract, ACTspMean, AGI, EAS, ECU, ECV, ENRf, EVNspCount, EVNspFract, EVNspMean, RAOQ, ROIcover, ROItotal, ROU, ZCR\nReason: These indices assess the intricacy and richness of the soundscape, often related to the diversity and variability of sound sources.\nDiversity Indices:\nDescription: Indices that measure the diversity of sounds.\nIncluded Indices: ADI, AEI, BI, BioEnergy, H_gamma, H_GiniSimpson, H_Havrda, H_pairedShannon, H_Renyi\nReason: These indices quantify the variety and abundance of different sounds, reflecting the ecological diversity and health of the environment.\nSpectral Indices:\nDescription: Indices that measure sound frequency characteristics.\nIncluded Indices: AnthroEnergy, EPS, ESP_KURT, EPS_SKEW, Hf, HFC, KURTf, LFC, MFC, MED, NBPEAKS, NDSI, rBA, SKEWf, VARf\nReason: These indices analyze the frequency content of sounds, providing insights into the spectral characteristics of the soundscape.\nTemporal Indices:\nDescription: Indices that measure changes in sound over time.\nIncluded Indices: ACTtCount, ACTtFraction, ACTtMean, BGNt, EVNtCount, EVNtFraction, EVNtMean, Ht, LEQt, MEANt, SKEWt, TFSD, VARt\nReason: These indices quantify how sound varies over time, capturing temporal patterns and dynamics in the soundscape.\n\nNOTE: for indices that required specification of frequency bands for calculations, we selected an imperfect representation for anthrophony and biophony. This is largely because of the overlap between anthropogenic noise and biological signals in the lower frequencies. So care should be taken when interpreting results. The bands for each dataset are broken out by the sampling rate of the dataset, as larger bandwidth allows for greater frequencies to use.\n16 kHz Datasets:\n\nFrequency thresholds: flim_low = 0-1,000 Hz | flim_mid = 1,000 - 4,000 Hz | flim_hi = 4,000 - 8,000 Hz\nSelect indices that required two bands (BI, NDSI, rBA, AnthroEnergy, BioEnergy, ROU, ADI, AEI): anthrophony band = 0-1,000 Hz | biophony band = 1,000 - 8,000 Hz\nCAVEAT: We are aware there is biological and anthro energy in these bands! We can modify in future iterations but had to make a decision for initial exploration.\nACI: Averaged over full bandwidth (also have ACI calculated at small band intervals, but not displayed in app)\n\nFull Bandwidth Datasets:\n\nFrequency thresholds: flim_low = 0-1,500 Hz | flim_mid = 1,500 - 8,000 Hz | flim_hi = 8,000 Hz - Nyquist\nSelect indices that required two bands (BI, NDSI, rBA, AnthroEnergy, BioEnergy, ROU, ADI, AEI): anthrophony band = 0-1,500 Hz | biophony band = 1,500 Hz - Nyquist\nCAVEAT: We are aware there is biological and anthro energy in these bands! We can modify in future iterations but had to make a decision for initial exploration.\nACI: Averaged over full bandwidth (also have ACI calculated at small band intervals, but not displayed in app)\n\n\n\n\nIndex Definitions\nBelow is a summary of the available spectro-temporal features, alpha acoustic indices, temporal features, and spectral features in scikit-maad:\nACI (Acoustic Complexity Index)\n\nDescription: Quantifies the complexity of sound by evaluating the variation in amplitude among frequency bands.\nFormula: \n\nADI (Acoustic Diversity Index)\n\nDescription: Measures the variety of sound frequencies present, indicative of biodiversity.\nFormula: frequency bin.\n\nAEI (Acoustic Evenness Index)\n\nDescription: Evaluates the evenness of the distribution of sound energy across frequencies.\nFormula: Similar to ecological evenness indices, calculated as AEI=\n\nACTspCount (Active Space Count)\n\nDescription: Count of spatial areas showing significant sound activity.\nFormula: Counting areas where sound exceeds a certain spatial or acoustic threshold.\n\nACTspFract (Active Space Fraction)\n\nDescription: Fraction of the spatial domain showing active sound production.\nFormula: \n\nACTspMean (Active Space Mean)\n\nDescription: Average level of sound activity across spatial areas.\nFormula: \n\nACTtCount (Active Time Count)\n\nDescription: Number of times the sound level exceeds the predefined threshold.\nFormula: Counting the number of active frames.\n\nACTtFraction (Active Time Fraction)\n\nDescription: Proportion of the recording duration where the sound level exceeds a predefined threshold.\nFormula: \n\nACTtMean (Active Time Mean)\n\nDescription: Average sound level during active times.\nFormula: \n\nAGI (Acoustic Gap Index)\n\nDescription: Index measuring the gaps or silent intervals within the acoustic signal, indicative of disturbance.\nFormula: \n\nAnthroEnergy (Anthropogenic Energy)\n\nDescription: Measure of energy associated with human-made sounds.\nFormula: Sum of energy in designated anthropogenic frequency bands.\n\nBGNf (Background Noise Level Frequency)\n\nDescription: Background noise level in the frequency domain.\nFormula: Typically estimated during periods of minimal activity; specifics can vary.\n\nBGNt (Background Noise Level Time)\n\nDescription: Level of background noise in the time domain.\nFormula: Typically estimated during periods of minimal activity; specifics can vary.\n\nBI (Biotic Index)\n\nDescription: Index evaluating the presence of biological sounds.\nFormula: Calculated as a function of specific frequency and time thresholds indicative of biological activity.\n\nBioEnergy (Biophonic Energy)\n\nDescription: Measure of energy associated with natural sounds.\nFormula: Sum of energy in designated biophonic frequency bands.\n\nEAS (Energy Acoustic Spectrum)\n\nDescription: Total acoustic energy measured across the spectrum.\nFormula: \n\nECU (Evenness of the Channel Utilization)\n\nDescription: Evenness with which different frequency channels are utilized.\nFormula: \n\nECV (Energy Coefficient of Variation)\n\nDescription: Coefficient of variation of the energy across different frequency bands.\nFormula: ​​\n\nENRf (Energy Ratio Frequency)\n\nDescription: Ratio of energy within certain frequency bands compared to the total energy.\nFormula: \n\nEPS (Energy Peak Spectrum)\n\nDescription: Measure of the peak energy in the spectrum.\nFormula: \n\nEPS_KURT (Energy Peak Spectrum Kurtosis)\n\nDescription: Kurtosis of the energy peak spectrum, indicating the shape of the peak distribution.\nFormula: Calculation of kurtosis applied to the distribution of peak energies.\n\nEPS_SKEW (Energy Peak Spectrum Skewness)\n\nDescription: Skewness of the energy peak spectrum, indicating the asymmetry of the peak distribution.\nFormula: Calculation of skewness applied to the distribution of peak energies.\n\nEVNspCount (Event Space Count)\n\nDescription: Count of sound events in spatial areas.\nFormula: Counting distinct sound events in spatial regions.\n\nEVNspFract (Event Space Fraction)\n\nDescription: Fraction of the spatial domain where sound events occur.\nFormula: \n\nEVNspMean (Event Space Mean)\n\nDescription: Average level of sound events across spatial areas.\nFormula: \n\nEVNtCount (Event Time Count)\n\nDescription: Number of distinct sound events detected.\nFormula: Counting distinct events based on a defined threshold.\n\nEVNtFraction (Event Time Fraction)\n\nDescription: Fraction of time that ‘events’ (heightened sound activity) occur.\nFormula: \n\nEVNtMean (Event Time Mean)\n\nDescription: Average sound level during event times.\nFormula: \n\nHf (High Frequency Coverage)\n\nDescription: Extent to which high frequencies are present in the soundscape.\nFormula: \n\nHFC (High Frequency Coverage)\n\nDescription: Reiterates the presence and extent of high frequencies in the soundscape.\nFormula: \n\nH_GiniSimpson (Gini-Simpson Entropy)\n\nDescription: Entropy based on the Gini-Simpson index, reflecting diversity and probability.\nFormula: \n\nH_Havrda (Havrda Entropy)\n\nDescription: Entropy measure based on Havrda-Charvat entropy, reflecting diversity.\nFormula: \n\nH_pairedShannon (Paired Shannon Entropy)\n\nDescription: Shannon entropy calculated from paired data sets for comparing diversity.\nFormula: \n\nH_Renyi (Renyi Entropy)\n\nDescription: Generalized entropy measure capturing diversity and richness of the soundscape.\nFormula: \n\nH_gamma (Gamma Entropy)\n\nDescription: Entropy measure based on the gamma distribution, used for sound diversity.\nFormula: Specific calculation details can vary, often involving the use of the gamma function in entropy calculations.\n\nKURTf (Kurtosis Frequency)\n\nDescription: ‘Tailedness’ of the frequency distribution, indicating infrequent extreme frequency deviations.\nFormula: \n\nKURTt (Kurtosis Time)\n\nDescription: ‘Tailedness’ of the amplitude distribution in the time domain, indicating infrequent extreme deviations.\nFormula: \n\nLFC (Low Frequency Coverage)\n\nDescription: Extent to which low frequencies are present in the soundscape.\nFormula: \n\nLEQf (Long-term Equivalent Level Frequency)\n\nDescription: Equivalent constant sound level in the frequency domain that conveys the same sound energy.\nFormula: \n\nLEQt (Long-term Equivalent Level Time)\n\nDescription: Constant sound level that delivers the same sound energy as the varying sound level over a specified period.\nFormula: \n\nMFC (Mid Frequency Coverage)\n\nDescription: Extent to which mid-range frequencies are present in the soundscape.\nFormula: \n\nMEANf (Mean Frequency)\n\nDescription: Average frequency of sounds, weighted by their amplitude.\nFormula: \n\nMEANt (Mean Time)\n\nDescription: The average amplitude of the audio signal over time, reflecting the overall loudness.\nFormula: \n\nNBPEAKS (Number of Peaks)\n\nDescription: Total number of prominent peaks in the frequency spectrum.\nFormula: Count of frequency peaks exceeding a certain amplitude threshold.\n\nNDSI (Normalized Difference Soundscape Index)\n\nDescription: Index of the balance between biological sounds and anthropogenic noise.\nFormula: \n\nRAOQ (Rao’s Quadratic Entropy)\n\nDescription: Entropy measure that considers both abundance and dissimilarity among categories.\nFormula: ​\n\nrBA (Relative Biophony-Anthrophony)\n\nDescription: Relative levels of biophony (natural sounds) and anthrophony (human-made sounds).\nFormula: Similar to NDSI, calculated for different contexts or specific frequency bands.\n\nROIcover (Region of Interest Coverage)\n\nDescription: Extent to which regions of interest cover the acoustic space.\nFormula: ​\n\nROItotal (Region of Interest Total)\n\nDescription: Total measure or count of regions of interest identified within the soundscape.\nFormula: Counting the total number of identified acoustic regions of interest.\n\nROU (Roughness)\n\nDescription: Measure of the texture or roughness of the sound profile.\nFormula: Typically involves calculating modulation of sound amplitude or frequency over time.\n\nSKEWf (Skewness Frequency)\n\nDescription: Asymmetry of the frequency distribution of the sound.\nFormula: \n\nSKEWt (Skewness Time)\n\nDescription: Asymmetry of the amplitude distribution of the audio signal in the time domain.\nFormula: \n\nSNRf (Signal-to-Noise Ratio Frequency)\n\nDescription: Ratio of signal level to noise level in the frequency domain.\nFormula: \n\nSNRt (Signal-to-Noise Ratio Time)\n\nDescription: Ratio of the audio signal level to the level of background noise.\nFormula: \n\nTFSD (Temporal Frequency Spectral Diversity)\n\nDescription: Diversity of frequencies over time, reflecting temporal variation.\nFormula: Often calculated using indices similar to biodiversity indices but applied to the temporal frequency spectrum.\n\nVARf (Variance Frequency)\n\nDescription: Variance in the frequency of sounds, indicating dispersion around the mean frequency.\nFormula: \n\nVARt (Variance Time)\n\nDescription: Variance of the time-domain audio signal amplitude, indicating amplitude fluctuations over time.\nFormula: \n\nZCR (Zero Crossing Rate)\n\nDescription: Measures the rate at which the signal changes from positive to negative or back, indicating the frequency content of the sound.\nFormula: \n\n\n\n\nFrequency Bands, Thresholds & Masks\n\nIndices Measuring Biophony vs. Anthrophony\n\nNDSI (Normalized Difference Soundscape Index)\n\nDescription: Quantifies the balance between biophonic and anthrophonic sound sources within a soundscape. It is often used as a metric for assessing environmental health and biodiversity.\nFormula: \n\nB: Biophony energy within defined frequency bands.\nA: Anthrophony energy within defined frequency bands.\n\nFrequency Limits:\n\nBiophony: Typically low to mid frequencies (e.g., 0-1000 Hz).\nAnthrophony: Typically higher frequencies (e.g., 1000-4000 Hz).\n\n\nBioEnergy (Biophonic Energy)\n\nDescription: Measures the total acoustic energy attributed to natural sound sources within the defined biophonic frequency range.\nCalculation: Sum of the acoustic energy in the biophonic frequency bands.\nFrequency Limits:\n\nBiophony: Typically low frequencies (e.g., 0-1000 Hz).\n\n\nAnthroEnergy (Anthropogenic Energy)\n\nDescription: Measures the total acoustic energy attributed to human-made sound sources within the defined anthrophonic frequency range.\nCalculation: Sum of the acoustic energy in the anthrophonic frequency bands.\nFrequency Limits:\n\nAnthrophony: Typically higher frequencies (e.g., 1000-4000 Hz).\n\n\nrBA (Relative Biophony-Anthrophony)\n\nDescription: Compares the relative levels of biophonic and anthrophonic energy within the soundscape.\nCalculation: A ratio or comparison metric between the biophonic and anthrophonic energies.\nFrequency Limits:\n\nBiophony: Typically low frequencies (e.g., 0-1000 Hz).\nAnthrophony: Typically higher frequencies (e.g., 1000-4000 Hz).\n\n\n\nIn addition to the indices that explicitly measure biophony and anthrophony (like NDSI, BioEnergy, AnthroEnergy, and rBA), there are other indices in the scikit-maad library that utilize these frequency limits (flim_low and flim_mid) for their calculations. Here are some additional indices that rely on these frequency bands:\n\nBI (Biotic Index)\n\nDescription: Measures the presence and intensity of biotic sounds within the biophonic frequency range.\nFrequency Limits:\n\nBiophony: Typically low frequencies (e.g., 0-1000 Hz).\n\n\nACI (Acoustic Complexity Index)\n\nDescription: Quantifies the complexity of the soundscape by evaluating the variation in amplitude among frequency bands over time.\nFrequency Limits: The index can be applied to specific frequency bands, often including the biophonic and anthrophonic ranges.\n\nBiophony: 0-1000 Hz.\nAnthrophony: 1000-4000 Hz.\n\n\nEPS (Energy Peak Spectrum)\n\nDescription: Measures the peak energy in the spectrum within specified frequency bands.\nFrequency Limits: Can be calculated separately for biophony and anthrophony bands.\n\nBiophony: 0-1000 Hz.\nAnthrophony: 1000-4000 Hz.\n\n\nROU (Roughness)\n\nDescription: Evaluates the texture or roughness of the sound profile, which can be calculated for different frequency bands.\nFrequency Limits:\n\nBiophony: 0-1000 Hz.\nAnthrophony: 1000-4000 Hz.\n\n\nADI (Acoustic Diversity Index)\n\nDescription: Measures the variety of sound frequencies present, indicative of biodiversity.\nFrequency Limits: Applied across biophony and anthrophony bands to assess diversity in each.\n\nBiophony: 0-1000 Hz.\nAnthrophony: 1000-4000 Hz.\n\n\nAEI (Acoustic Evenness Index)\n\nDescription: Evaluates the evenness of the distribution of sound energy across frequencies, which can be applied to specific bands.\nFrequency Limits:\n\nBiophony: 0-1000 Hz.\nAnthrophony: 1000-4000 Hz.\n\n\n\n\n\nFrequency Band Details\nThe differentiation between biophony and anthrophony is critical for accurately interpreting these indices. Here’s how these limits are typically set in the scikit-maad functions:\n\nBiophony (Natural Sounds):\n\nFrequency bands often range from 0 Hz to 1000 Hz. This range captures most natural sounds such as bird calls, frog croaks, and other wildlife communications.\n\nAnthrophony (Human-made Sounds):\n\nFrequency bands often range from 1000 Hz to 4000 Hz. This range includes many anthropogenic noises like machinery, vehicles, and urban sounds.\n\n\nExample Code for Frequency Limits in scikit-maad\nHere’s how these frequency limits are typically applied in the scikit-maad functions:\n```{python}} spectral_indices, spectral_indices_per_bin = features.all_spectral_alpha_indices( Sxx_power=Sxx_power, tn=tn, fn=fn, flim_low=[0, 1000], # Frequency band for biophony flim_mid=[1000, 4000], # Frequency band for anthrophony flim_hi=[4000, 8000], # Higher frequencies if needed gain=G, sensitivity=S, verbose=False, R_compatible=‘soundecology’, mask_param1=6, mask_param2=0.5, display=False )\n\n#### Thresholds and Masks in `scikit-maad`\n\n1.  **dB_threshold**\n\n    **Description**: This threshold is used to determine active segments in the audio signal. It sets the minimum amplitude level that must be exceeded for a segment to be considered active.\n\n    -   **Use Case**: Applied in the calculation of temporal indices like `ACTtCount`, `ACTtFraction`, and other active time metrics.\n\n    -   **Parameters**:\n\n        -   **dB_threshold**: 3 dB \\# Minimum amplitude level for active segments\n\n    -   **Example in Code**\n\n```{python}}\ntemporal_indices = features.all_temporal_alpha_indices(\n    s=segment_wave,\n    fs=fs,\n    gain=G,\n    sensibility=S,\n    dB_threshold=3,  # This is the threshold defined in decibels\n    rejectDuration=0.01,\n    verbose=False,\n    display=False\n)\n\nmask_param1\nDescription: This parameter is typically used as a threshold in decimbels for masking low-energy components in the spectrogram. Any part of the spectrogram with energy below this threshold might be set to zero or ignored in further calculations.\n\nUse Case: Used in spectral analysis to filter out noise and irrelevant low-energy componenets.\nParameters: mask_param1: 6 dB # Threshold for masking low-energy components in the spectrogram\nExample in Code:\n```{python}} spectral_indices, spectral_indices_per_bin = features.all_spectral_alpha_indices( Sxx_power=Sxx_power, tn=tn, fn=fn, flim_low=[0, 1000], flim_mid=[1000, 4000], flim_hi=[4000, 8000], gain=G, sensitivity=S, verbose=False, R_compatible=‘soundecology’, mask_param1=6, # This is the threshold for masking in decibels mask_param2=0.5, display=False )\n```\n\nmask_param2\nDescription: This parameter might determine the sensitivity or proportion of the masking process, influencing how aggressively the mask is applied.\n\nUse Case: Used alongside mask_param1 to refine the masking process in spectral analysis.\nParameters: mask_param2: 0.5 # Sensitivity or proportion parameter for the masking process\n\n\nExample code\n```{python}} spectral_indices, spectral_indices_per_bin = features.all_spectral_alpha_indices( Sxx_power=Sxx_power, tn=tn, fn=fn, flim_low=[0, 1000], flim_mid=[1000, 4000], flim_hi=[4000, 8000], gain=G, sensitivity=S, verbose=False, R_compatible=‘soundecology’, mask_param1=6, mask_param2=0.5, # This is the sensitivity parameter for the mask display=False )\n```\nThese parameters affect a suite of indices, and so users should consider the intended spectral components they are interested in exploring.",
    "crumbs": [
      "Acoustic Indices"
    ]
  },
  {
    "objectID": "recommendations.html",
    "href": "recommendations.html",
    "title": "Challenges & Future Recommendations",
    "section": "",
    "text": "Existing Exploratory Dataset\n\nChallenges\nThe compilation of multiple datasets with varied duty cycle, sampling rates, naming conventions, and file duration posed several challenges that should be considered for future efforts.\n\nFile-naming Convention: Most entities have a custom file name that was used for generating audio files. The scikit-maad code required specific formats for writing date and time into the csv data products. This required custom code for each dataset.\nDecimating & Splitting: Processing locally with a month of data at a time had limitations in terms of the volume of indices that were collected in this study. Audio files longer than 5 minutes require segmentation to allow for processing (crashing resulted with 25-minute and 5-hour files). Additionally, the continuous data at 5-minute intervals was unrealistic to process at very small duration scales (e.g., 10 and 30 seconds).\nAnnotation Resolution: While the Key West and May River datasets provided annotations at a higher resolution, the SanctSound-HI01 and Gray’s Reef sanctuary annotations of biophony were too coarse a resolution to use for meaningful\nProcessing Times: Local processing was used for this effort which is required long duration, high computing effort for multiple runs.\n\n\n\nRecommendations\nDespite the progress made in this exploratory study, clearer insights into the factors influencing acoustic-based indices is required.\n\nFinalize list of visualizations and comparisons including incorporating a panel on comparison of audio durations for each site and a vessel versus non-vessel comparison of indices.\nThe data provided should be reviewed and assessed for notable trends within the data that are either specific to each site or common across sites. These trends should then be further examined through statistical comparisons.\nBiophony Annotations: Biological contributions to the data are likely of great importance for better understanding and elucidating any possible trends in the data. For instance, the Chuckchi Sea dataset has extensive vocal activity, presumably from bearded seals, and the OOI and ONC data have limited biological activity from a cursory review. Annotation to the level of detail that is found in the Key West dataset would allow for compilation of volume of acoustic activity by species, which could be informative in this study.\n\n\n\n\nScaling to Larger Datasets\n\nChallenges\nThis exploratory study revealed several challenges that are anticipating for scaling this to a larger dataset.\n\nFile Management: File management is a necessary step but not one that can be required of data collectors. File renaming, segmentation and decimation are all likely necessary to further explore acoustic indices with larger datasets.\nAnnotations: Processing data to understand the biophony and anthrophony within the datasets is a likely necessary step. A high resolution in annotations is seemingly necessary for this effort.\nProcessing is limited in local environments, and requires high computational intensive equipment.\n\n\n\nRecommendations\nWe highly recommend navigating this project to a cloud-based scenario, and directly querying data from data collector repositories. This would require cloud engineer design for the data query, stepping through file management and pre-processing, compiling and visualizing/analyzing data in a dashboard. We recommend an Amazon Warehouse Services (AWS) approach in the future.",
    "crumbs": [
      "Recommendations"
    ]
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Evaluating Indices",
    "section": "",
    "text": "Diel Relationships\nTBD\n\n\nIndices vs. Annotations\nTBD\n\n\nRelationship with Environmental Data\nTBD",
    "crumbs": [
      "Evaluating Indices"
    ]
  }
]